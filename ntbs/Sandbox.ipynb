{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5bff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Consts\n",
    "SEED = 42\n",
    "import struct as st\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae164654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to parse IDX files\n",
    "def parse_idx(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        magic = st.unpack('>I', file.read(4))[0]  # Magic number (4 bytes)\n",
    "        num_items = st.unpack('>I', file.read(4))[0]  # Number of items (4 bytes)\n",
    "\n",
    "        if magic == 2051:  # Magic number for images\n",
    "            num_rows = st.unpack('>I', file.read(4))[0]\n",
    "            num_cols = st.unpack('>I', file.read(4))[0]\n",
    "            num_bytes = num_items * num_rows * num_cols\n",
    "            data = np.frombuffer(file.read(num_bytes), dtype=np.uint8)\n",
    "            return data.reshape(num_items, num_rows, num_cols)\n",
    "        elif magic == 2049:  # Magic number for labels\n",
    "            data = np.frombuffer(file.read(num_items), dtype=np.uint8)\n",
    "            return data\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown magic number: {magic}\")\n",
    "\n",
    "# Parse the training data\n",
    "x_train = parse_idx('DigitData/train-images.idx3-ubyte')\n",
    "y_train = parse_idx('DigitData/train-labels.idx1-ubyte')\n",
    "\n",
    "x_test = parse_idx('DigitData/t10k-images.idx3-ubyte')\n",
    "y_test = parse_idx('DigitData/t10k-labels.idx1-ubyte')\n",
    "\n",
    "# Reshape and scale down\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444a6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 787)\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "train_mean = np.mean(x_train, axis=0)\n",
    "train_std = np.std(x_train, axis=0) + 1e-12\n",
    "\n",
    "x_train = (x_train - train_mean) / train_std\n",
    "x_test = (x_test - train_mean) / train_std\n",
    "\n",
    "# Add extra ones column\n",
    "x_train = np.hstack([x_train, np.ones((len(x_train), 1)) ])\n",
    "x_test = np.hstack([ x_test, np.ones((len(x_test), 1)) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ce8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(60000, 787) (60000, 10) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Full Probability Matrix\n",
    "def Pr(X, theta):\n",
    "    \"\"\" \n",
    "    X: (n, d)\n",
    "    theta: (d, c)\n",
    "    \"\"\"\n",
    "    logits = X @ theta\n",
    "    logits = np.hstack([logits, np.zeros((X.shape[0], 1))]) # Add Last class\n",
    "    print(logits.shape)\n",
    "    logits -= logits.max(axis=1, keepdims=True)\n",
    "    # logits = np.clip(logits, -500, 500)\n",
    "    exp = np.exp(logits)\n",
    "    row_sums = np.sum(exp, axis=1, keepdims=True)\n",
    "    return exp / row_sums\n",
    "\n",
    "# One Hot Encoding\n",
    "def one_hot(Y, d=10):\n",
    "    \"\"\"\n",
    "    Y: (n) -> (n, d) \n",
    "    \"\"\"\n",
    "    Y_oh = np.zeros(shape=(len(Y), d), dtype=np.int64)\n",
    "    idxs = np.arange(start=0, stop=len(Y), step=1)\n",
    "    Y_oh[idxs, Y] = 1\n",
    "    return Y_oh \n",
    "\n",
    "def jacobian(X, y_onehot, P):\n",
    "    total = X.T @ (y_onehot - P)\n",
    "    return total[:, :-1]\n",
    "\n",
    "theta = np.random.randn(787, 9)\n",
    "P = Pr(x_train, theta)\n",
    "y_onehot = one_hot(y_train)\n",
    "J = jacobian(x_train, y_onehot, P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c3ac848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjugate Gradient Helper\n",
    "def H_dot(X, P, V):\n",
    "    \"\"\"\n",
    "    X: (n, d)\n",
    "    P: (n, c) --> doesn't contain probability of last class\n",
    "    V: (d * c)\n",
    "    \"\"\"\n",
    "    d = X.shape[1]\n",
    "    c = P.shape[1]\n",
    "    V = V.reshape(shape=(d, c))\n",
    "\n",
    "    \n",
    "\n",
    "    Z = X @ V.T\n",
    "    A = P * Z\n",
    "    s = A.sum(axis=1, keepdims=True)\n",
    "    Z_next = A - P * s\n",
    "    Hv = (X.T @ Z_next).T\n",
    "    return np.reshape(Hv, d * c)\n",
    "\n",
    "def conj_grad(X, P, theta, g):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LogisticRegression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
