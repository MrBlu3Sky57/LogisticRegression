{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 905,
   "id": "ef5bff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Consts\n",
    "SEED = 42\n",
    "import numpy as np\n",
    "from util import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "ae164654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read data and construct data sets\n",
    "NAME = \"uciml/iris\"\n",
    "data = get_data(\"uciml/iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "4444a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[1:]\n",
    "xs = data[:, 1:-1]\n",
    "xs = xs.astype(float)\n",
    "ys = data[:, -1]\n",
    "stoi = {y: i for i, y in enumerate(list(set(ys)))}\n",
    "for i in range(ys.size):\n",
    "    ys[i] = stoi[ys[i]]\n",
    "ys = ys.astype(int)\n",
    "itos = {stoi[y]: y for y in stoi}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "47906abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "np.random.seed(SEED)\n",
    "idxs = np.random.permutation(len(xs))\n",
    "\n",
    "xs = xs[idxs]\n",
    "ys = ys[idxs]\n",
    "\n",
    "# Split Data\n",
    "x_train = xs[:120]\n",
    "y_train = ys[:120]\n",
    "x_test = xs[120:]\n",
    "y_test = ys[120:]\n",
    "\n",
    "# Normalizing data\n",
    "train_mean = np.mean(x_train, axis=0)\n",
    "train_std = np.std(x_train, axis=0) + 1e-12\n",
    "\n",
    "x_train = (x_train - train_mean) / train_std\n",
    "x_test = (x_test - train_mean) / train_std\n",
    "\n",
    "# Extra column for bias term\n",
    "x_train = np.hstack([x_train, np.ones((len(x_train), 1)) ])\n",
    "x_test = np.hstack([ x_test, np.ones((len(x_test), 1)) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "id": "2b5ce8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Probability Matrix\n",
    "def Pr(X, theta):\n",
    "    \"\"\" \n",
    "    X: (n, d)\n",
    "    theta: (d, c)\n",
    "    \"\"\"\n",
    "    logits = X @ theta\n",
    "    logits = np.hstack([logits, np.zeros((X.shape[0], 1))]) # Add Last class\n",
    "    logits -= logits.max(axis=1, keepdims=True)\n",
    "    # logits = np.clip(logits, -500, 500)\n",
    "    exp = np.exp(logits)\n",
    "    row_sums = np.sum(exp, axis=1, keepdims=True)\n",
    "    return exp / row_sums\n",
    "\n",
    "# One Hot Encoding\n",
    "def one_hot(Y, d):\n",
    "    \"\"\"\n",
    "    Y: (n) -> (n, d) \n",
    "    \"\"\"\n",
    "    Y_oh = np.zeros(shape=(len(Y), d), dtype=np.int64)\n",
    "    idxs = np.arange(start=0, stop=len(Y), step=1)\n",
    "    Y_oh[idxs, Y] = 1\n",
    "    return Y_oh \n",
    "\n",
    "def jacobian(X, y_onehot, P):\n",
    "    total = X.T @ (y_onehot - P)\n",
    "    return total[:, :-1]\n",
    "\n",
    "def cross_entropy(P, Y_oh):\n",
    "    return -np.mean(np.sum(Y_oh * np.log(P + 1e-12), axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "4c3ac848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjugate Gradient Helper\n",
    "def H_dot(X, P, V, _lambda):\n",
    "    \"\"\"\n",
    "    X: (n, d)\n",
    "    P: (n, c) --> doesn't contain probability of last class\n",
    "    V: (d * c)\n",
    "    \"\"\"\n",
    "    d = X.shape[1]\n",
    "    c = P.shape[1]\n",
    "    V = np.reshape(V, shape=(d, c))\n",
    "\n",
    "    Z = X @ V\n",
    "    A = P * Z\n",
    "    s = A.sum(axis=1, keepdims=True)\n",
    "    ZW = A - P * s\n",
    "    Hv = X.T @ ZW \n",
    "    Hv += _lambda * V\n",
    "    return Hv.flatten()\n",
    "\n",
    "def conj_grad(X, P, g, _lambda, tol=1e-15):\n",
    "    d = X.shape[1]\n",
    "    c = P.shape[1]\n",
    "    res = g\n",
    "    p = g\n",
    "    k = 0\n",
    "    x = np.zeros(g.shape)\n",
    "    while k < g.shape[0]:\n",
    "        Ap =  H_dot(X, P, p, _lambda)\n",
    "        rr = res @ res\n",
    "        alpha = (res @ res) / (p @ Ap + 1e-12)\n",
    "        x += alpha * p\n",
    "        res -= alpha * Ap\n",
    "\n",
    "        if np.linalg.norm(res) < tol:\n",
    "            break\n",
    "        beta = (res @ res) / (rr + 1e-12)\n",
    "        p = res + beta * p\n",
    "        k += 1\n",
    "    return np.reshape(x, shape=(d, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "id": "5ddd62b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, loss: 0.3066\n",
      "Epoch 50, loss: 0.1928\n",
      "Epoch 75, loss: 0.1606\n",
      "Epoch 100, loss: 0.1482\n",
      "Epoch 125, loss: 0.1427\n",
      "Epoch 150, loss: 0.1402\n",
      "Epoch 175, loss: 0.1390\n",
      "Epoch 200, loss: 0.1384\n",
      "Epoch 225, loss: 0.1381\n",
      "Epoch 250, loss: 0.1379\n",
      "Epoch 275, loss: 0.1378\n",
      "Epoch 300, loss: 0.1378\n"
     ]
    }
   ],
   "source": [
    "def train(X, Y, epochs=300, _lambda=1e-4, _alpha=0.15):\n",
    "    C = np.unique(Y).size\n",
    "    d = X.shape[1]\n",
    "    y_onehot = one_hot(Y, C)\n",
    "    thetas = np.random.randn(X.shape[1], C-1)\n",
    "\n",
    "    # To Shuffle Batches:\n",
    "    gen = np.random.default_rng(SEED)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 25 == 0:\n",
    "            _alpha /= 2\n",
    "        idx = gen.permutation(len(X))\n",
    "        X_epoch = X[idx]\n",
    "        Y_epoch = y_onehot[idx]\n",
    "\n",
    "        P = Pr(X_epoch, thetas)\n",
    "        J = jacobian(X_epoch, Y_epoch, P)\n",
    "        J[:-1, :] += _lambda * thetas[:-1, :]\n",
    "        J = np.reshape(J, shape=(d * (C-1)))\n",
    "\n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            loss = cross_entropy(P, Y_epoch)\n",
    "            print(f\"Epoch {epoch + 1}, loss: {loss:.4f}\")\n",
    "\n",
    "        P = P[:, :-1]\n",
    "        thetas += _alpha * conj_grad(X_epoch, P, J, _lambda)\n",
    "    return thetas\n",
    "thetas = train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "a9b3cb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9136649514008005\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "def predict(xs, thetas):\n",
    "    probs = Pr(xs, thetas)\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    return preds\n",
    "\n",
    "def acc(xs, ys, thetas):\n",
    "    preds = predict(xs, thetas)\n",
    "    preds = preds.astype(int)\n",
    "    return np.mean(preds == ys)\n",
    "\n",
    "print(acc(x_test, y_test, thetas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "c50b24cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from local file\n",
    "X_train = np.loadtxt(\"data/PenData/pendigits.tra\", delimiter=\",\")\n",
    "X_test = np.loadtxt(\"data/PenData/pendigits.tes\", delimiter=\",\")\n",
    "\n",
    "# Split features and labels\n",
    "x_train, y_train = X_train[:, :-1], X_train[:, -1].astype(int)\n",
    "x_test, y_test = X_test[:, :-1], X_test[:, -1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "bb678427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data\n",
    "train_mean = np.mean(x_train, axis=0)\n",
    "train_std = np.std(x_train, axis=0) + 1e-12\n",
    "\n",
    "x_train = (x_train - train_mean) / train_std\n",
    "x_test = (x_test - train_mean) / train_std\n",
    "\n",
    "# Extra column for bias term\n",
    "x_train = np.hstack([x_train, np.ones((len(x_train), 1)) ])\n",
    "x_test = np.hstack([ x_test, np.ones((len(x_test), 1)) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "6345b8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, loss: 0.2648\n",
      "Epoch 50, loss: 0.1715\n",
      "Epoch 75, loss: 0.1451\n",
      "Epoch 100, loss: 0.1351\n",
      "Epoch 125, loss: 0.1308\n",
      "Epoch 150, loss: 0.1287\n",
      "Epoch 175, loss: 0.1278\n",
      "Epoch 200, loss: 0.1273\n",
      "Epoch 225, loss: 0.1271\n",
      "Epoch 250, loss: 0.1269\n",
      "Epoch 275, loss: 0.1269\n",
      "Epoch 300, loss: 0.1269\n",
      "0.9222412807318467\n"
     ]
    }
   ],
   "source": [
    "thetas = train(x_train, y_train, _lambda=1e-4)\n",
    "print(acc(x_test, y_test, thetas))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LogisticRegression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
